@inproceedings{conference-paper-cikm2020,
  author = {Roitero, Kevin and Soprano, Michael and Portelli, Beatrice and Spina, Damiano and Della Mea, Vincenzo and Serra, Giuseppe and Mizzaro, Stefano and Demartini, Gianluca},
  title = {The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation Objectively?},
  year = {2020},
  isbn = {9781450368599},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3340531.3412048},
  doi = {10.1145/3340531.3412048},
  abstract = {Misinformation is an ever increasing problem that is difficult to solve for the research community and has a negative impact on the society at large. Very recently, the problem has been addressed with a crowdsourcing-based approach to scale up labeling efforts: to assess the truthfulness of a statement, instead of relying on a few experts, a crowd of (non-expert) judges is exploited. We follow the same approach to study whether crowdsourcing is an effective and reliable method to assess statements truthfulness during a pandemic. We specifically target statements related to the COVID-19 health emergency, that is still ongoing at the time of the study and has arguably caused an increase of the amount of misinformation that is spreading online (a phenomenon for which the term "infodemic" has been used). By doing so, we are able to address (mis)information that is both related to a sensitive and personal issue like health and very recent as compared to when the judgment is done: two issues that have not been analyzed in related work.In our experiment, crowd workers are asked to assess the truthfulness of statements, as well as to provide evidence for the assessments as a URL and a text justification. Besides showing that the crowd is able to accurately judge the truthfulness of the statements, we also report results on many different aspects, including: agreement among workers, the effect of different aggregation functions, of scales transformations, and of workers background / bias. We also analyze workers behavior, in terms of queries submitted, URLs found / selected, text justifications, and other behavioral data like clicks and mouse actions collected by means of an ad hoc logger.},
  booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM2020). Galway, Ireland (Online). October 19-23, 2020. Conference Rank: GGS A+, Core A},
  pages = {1305–1314},
  numpages = {10},
  keywords = {ordinal classification, infodemic, information behavior, covid-19, crowdsourcing},
  location = {Virtual Event, Ireland},
  series = {CIKM '20}
}

@inproceedings{conference-paper-sigir2020,
  author = {Roitero, Kevin and Soprano, Michael and Fan, Shaoyang and Spina, Damiano and Mizzaro, Stefano and Demartini, Gianluca},
  title = {Can The Crowd Identify Misinformation Objectively? The Effects of Judgment Scale and Assessor's Background},
  year = {2020},
  isbn = {9781450380164},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3397271.3401112},
  doi = {10.1145/3397271.3401112},
  abstract = {Truthfulness judgments are a fundamental step in the process of fighting misinformation, as they are crucial to train and evaluate classifiers that automatically distinguish true and false statements. Usually such judgments are made by experts, like journalists for political statements or medical doctors for medical statements. In this paper, we follow a different approach and rely on (non-expert) crowd workers. This of course leads to the following research question: Can crowdsourcing be reliably used to assess the truthfulness of information and to create large-scale labeled collections for information credibility systems? To address this issue, we present the results of an extensive study based on crowdsourcing: we collect thousands of truthfulness assessments over two datasets, and we compare expert judgments with crowd judgments, expressed on scales with various granularity levels. We also measure the political bias and the cognitive background of the workers, and quantify their effect on the reliability of the data provided by the crowd.},
  booktitle = {Proceedings of the 43st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2020). Xi’an, China (Online). July 25-30, 2020. Conference Rank: GGS A++, Core A*},
  pages = {439–448},
  numpages = {10},
  keywords = {classification, crowdsourcing, information credibility},
  location = {Virtual Event, China},
  series = {SIGIR '20}
}

@inproceedings{workshop-paper-iir2019,
  author = {Kevin Roitero andStefano Mizzaro andMichael Soprano},
  editor = {Maristella Agosti and Emanuele Di Buccio and Massimo Melucci and Stefano Mizzaro and Gabriella Pasi and Fabrizio Silvestri},
  title = {Bias and Fairness in Effectiveness Evaluation by Means of Network Analysis and Mixture Models},
  booktitle = {Proceedings of the 10th Italian Information Retrieval Workshop, Padova, Italy, September 16-18, 2019},
  series = {{CEUR} Workshop Proceedings},
  volume = {2441},
  pages = {6--7},
  publisher = {CEUR-WS.org},
  year = {2019},
  url = {http://ceur-ws.org/Vol-2441/paper4.pdf},
  timestamp = {Thu, 21 Jan 2021 17:36:26 +0100},
  biburl = {https://dblp.org/rec/conf/iir/RoiteroMS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{workshop-paper-birnld2019,
  author = {Michael Soprano and Kevin Roitero and Stefano Mizzaro},
  editor = {Muthu Kumar Chandrasekaran and Philipp Mayr},
  title = {{HITS} Hits Readersourcing: Validating Peer Review Alternatives Using Network Analysis},
  booktitle = {Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries {(BIRNDL} 2019) co-located with the 42nd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval {(SIGIR} 2019), Paris, France, July 25, 2019},
  series = {{CEUR} Workshop Proceedings},
  volume = {2414},
  pages = {70--82},
  publisher = {CEUR-WS.org},
  year = {2019},
  url = {http://ceur-ws.org/Vol-2414/paper7.pdf},
}

@inproceedings{conference-paper-ircdl2019,
  author = {Michael Soprano and Stefano Mizzaro},
  editor = {Paolo Manghi and Leonardo Candela and Gianmaria Silvello},
  title = {Crowdsourcing Peer Review: As We May Do},
  booktitle = {Digital Libraries: Supporting Open Science - 15th Italian Research Conference on Digital Libraries, {IRCDL} 2019, Pisa, Italy, January 31 - February 1, 2019, Proceedings},
  series = {Communications in Computer and Information Science},
  volume = {988},
  pages = {259--273},
  publisher = {Springer},
  year = {2019},
  url = {https://doi.org/10.1007/978-3-030-11226-4\_21},
  doi = {10.1007/978-3-030-11226-4\_21},
}

@inproceedings{conference-paper-aiudc2019,
  author = {Soprano, Michael and Mizzaro, Stefano},
  title = {Crowdsourcing Peer Review in the Digital Humanities?},
  year = {2019},
  isbn = {9788894253535},
  url = {http://aiucd2019.uniud.it/wp-content/uploads/2020/03/AIUCD2019-BoA_DEF.pdf},
  abstract = {We propose an alternative approach to the standard peer review activity that aims to exploit the otherwise lost opinions of readers of publications which is called Readersourcing, originally proposed by Mizzaro [1]. Such an approach can be formalized by means of different models which share the same general principles. These models should be able to define a way, to measure the overall quality of a publication as well the reputation of a reader as an assessor; moreover, from these measures it should be possible to derive the reputation of a scholar as an author. We describe an ecosystem called Readersourcing 2.0 which provides an implementation for two Readersourcing models [2, 3] by outlining its goals and requirements. Readersourcing 2.0 will be used in the future to gather fresh data to analyze and validate.},
  booktitle = {Book of Abstracts, 8th AIUCD Conference 2019 – Pedagogy, Teaching, and Research in the Age of Digital Humanities, page 251. Udine, Italy, 2019},
  pages = {251},
  location = {Udine, Italy},
  series = {AIUCD '19}
}

@inproceedings{conference-paper-sigir2018,
  author = {Roitero, Kevin and Soprano, Michael and Mizzaro, Stefano},
  title = {Effectiveness Evaluation with a Subset of Topics: A Practical Approach},
  year = {2018},
  isbn = {9781450356572},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209978.3210108},
  doi = {10.1145/3209978.3210108},
  abstract = {Several researchers have proposed to reduce the number of topics used in TREC-like initiatives. One research direction that has been pursued is what is the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way. Such a research direction has been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. We propose such a practical criterion for topic selection: we rely on the methods for automatic system evaluation without relevance judgments, and by running some experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than random topics.},
  booktitle = {Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018). Ann Arbor Michigan, U.S.A, July 8-12, 2018. Conference Rank: GGS A++, Core A*},
  pages = {1145–1148},
  numpages = {4},
  keywords = {topic selection, trec, few topics, test collections},
  location = {Ann Arbor, MI, USA},
  series = {SIGIR '18}
}

@article{journal-paper-jdiq2018,
  author = {Kevin Roitero and Michael Soprano and Andrea Brunello and Stefano Mizzaro},
  title = {Reproduce and Improve: An Evolutionary Approach to Select a Few Good Topics for Information Retrieval Evaluation},
  journal = {ACM Journal of Data and Information Quality - Special Issue on Reproducibility in IR: Evaluation Campaigns, Collections and Analyses (JDIQ), 10(3):12:1–12:21, September 2018. Journal Rank: Scimago Q2 (2018)},
  volume = {10},
  number = {3},
  pages = {12:1--12:21},
  year = {2018},
  url = {https://doi.org/10.1145/3239573},
  doi = {10.1145/3239573},
  timestamp = {Fri, 18 Sep 2020 11:17:40 +0200},
}
