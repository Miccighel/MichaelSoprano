[{"authors":null,"categories":null,"content":"Hello! I am Michael Soprano. Currently, I am working as a PhD Student at the University of Udine. I have worked there also as a Research Fellow. I got my Master Degree in Computer Science on 15th March 2018 and my Bachelor Degree in Multimedia and Web Technologies on 19th March 2015.\nMy research interests consist in Information Retrieval and Crowdsourcing. In particular, the use of crowdsourcing based approaches to address the increasing amount of misinformation that is spreading online. My long term goal is to build a human-in-the-loop system to cope with misinformation by measuring information truthfulness in real-time using crowd-powered data, human intelligence, and machine learning techniques.\nMore details can be found in my Research Statement.\n  Download my Curriculum Vitae in English or Italian and have a look at my Master, Bachelor, or High School Thesis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624286683,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am Michael Soprano. Currently, I am working as a PhD Student at the University of Udine. I have worked there also as a Research Fellow. I got my Master Degree in Computer Science on 15th March 2018 and my Bachelor Degree in Multimedia and Web Technologies on 19th March 2015.","tags":null,"title":"Michael Soprano","type":"authors"},{"authors":["Michael Soprano"],"categories":["teaching"],"content":"Teachers  Stefano Mizzaro - Theory Michael Soprano - Laboratory  Aims The aim of the course is to provide to the student the foundational knowledge and the practical skills concerning the area of social informatics. The course discusses both the so-called Social Media (Facebook, Twitter, etc.) and the Crowdsourcing phenomenon. In the first case, social behavior is supported by computational systems; in the second case, computational systems are supported by social behavior. The course deals with conceptual topics, theoretical foundations, and practical applications.\nTable of Contents My contribution to the course consisted in teaching practical applications of the concepts outlined by prof. Stefano Mizzaro for a total of 6 lectures (12 hours). The course\u0026rsquo;s language is Italian.\n Lesson 1: Python / Rest: Richiami Lesson 2: Twitter API: Tweepy Lesson 3: Twitter API: Endpoint V2 Lesson 4: Grafi: NetworkX Lesson 5: Crowd_Frame  ","date":1647601200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"de1d56014f1945e53d5be6adc54ac74a","permalink":"https://michaelsoprano.com/post/social-computing-2021_2022/","publishdate":"2022-03-18T11:00:00Z","relpermalink":"/post/social-computing-2021_2022/","section":"post","summary":"Bachelor's Degree in Internet of Things, Big Data \u0026 Web at the University of Udine. Academic Year 2020/2021. Lectures: 6. Hours: 12","tags":["Python","Jupyter","Twitter API","Tweepy","Postman","Graph","NetworkX","PyViz","Amazon Mechanical Turk","Angular","Crowdsourcing","Data Analysis"],"title":"Social Computing - 2021/2022","type":"post"},{"authors":["Michael Soprano"],"categories":null,"content":"","date":1645574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"4f0016a16cfec1e872ab8b5f250b1a75","permalink":"https://michaelsoprano.com/talk/crowd_frame-a-simple-and-complete-framework-to-deploy-complex-crowdsourcing-tasks-off-the-shelf/","publishdate":"2022-02-23T00:00:00Z","relpermalink":"/talk/crowd_frame-a-simple-and-complete-framework-to-deploy-complex-crowdsourcing-tasks-off-the-shelf/","section":"event","summary":"Conference Talk - The 15th ACM International Conference on Web Search and Data Mining. Held remotely due to COVID-19 (pre-recorded contribution).","tags":["crowdsourcing","framework","user behavior"],"title":"Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-Shelf","type":"event"},{"authors":["Michael Soprano","Kevin Roitero","Francesco Bombassei De Bona","Stefano Mizzaro"],"categories":[],"content":"","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"ecc886a321c43b35f809279214172822","permalink":"https://michaelsoprano.com/publication/conference-paper-wsdm-2022/","publishdate":"2022-02-15T14:36:35.998652Z","relpermalink":"/publication/conference-paper-wsdm-2022/","section":"publication","summary":"Due to their relatively low cost and ability to scale, crowdsourcing based approaches are widely used to collect a large amount of human annotated data. To this aim, multiple crowdsourcing platforms exist, where requesters can upload tasks and workers can carry them out and obtain payment in return. Such platforms share a task design and deploy workflow that is often counter-intuitive and cumbersome. To address this issue, we propose Crowd_Frame, a simple and complete framework which allows to develop and deploy diverse types of complex crowdsourcing tasks in an easy and customizable way. We show the abilities of the proposed framework and we make it available to researchers and practitioners.","tags":["framework","crowdsourcing","user behavior"],"title":"Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-Shelf","type":"publication"},{"authors":["Michael Soprano","Kevin Roitero","David La Barbera","Davide Ceolin","Damiano Spina","Stefano Mizzaro"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635170515,"objectID":"cdbd712d513d7625c4ef66c448f0c3ed","permalink":"https://michaelsoprano.com/publication/journal-paper-ipm-2021/","publishdate":"2021-09-15T15:00:00+01:00","relpermalink":"/publication/journal-paper-ipm-2021/","section":"publication","summary":"Recent work has demonstrated the viability of using crowdsourcing as a tool for evaluating the truthfulness of public statements. Under certain conditions such as: (1) having a balanced set of workers with different backgrounds and cognitive abilities; (2) using an adequate set of mechanisms to control the quality of the collected data; and (3) using a coarse grained assessment scale, the crowd can provide reliable identification of fake news. However, fake news are a subtle matter: statements can be just biased (“cherrypicked”), imprecise, wrong, etc. and the unidimensional truth scale used in existing work cannot account for such differences. In this paper we propose a multidimensional notion of truthfulness and we ask the crowd workers to assess seven different dimensions of truthfulness selected based on existing literature: Correctness, Neutrality, Comprehensibility, Precision, Completeness, Speaker’s Trustworthiness, and Informativeness. We deploy a set of quality control mechanisms to ensure that the thousands of assessments collected on 180 publicly available fact-checked statements distributed over two datasets are of adequate quality, including a custom search engine used by the crowd workers to find web pages supporting their truthfulness assessments. A comprehensive analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments are reliable when compared to an expert-provided gold standard; (2) the proposed dimensions of truthfulness capture independent pieces of information; (3) the crowdsourcing task can be easily learned by the workers; and (4) the resulting assessments provide a useful basis for a more complete estimation of statement truthfulness.","tags":["truthfulness","crowdsourcing","misinformation","explainability"],"title":"The Many Dimensions of Truthfulness: Crowdsourcing Misinformation Assessments on a Multidimensional Scale","type":"publication"},{"authors":["Erik Brand","Michael Soprano","Kevin Roitero","Michael Soprano","Gianluca Demartini"],"categories":[],"content":"","date":1634342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"8806a23701d3f3ffc0c1fd5cbd03cdcb","permalink":"https://michaelsoprano.com/publication/conference-paper-tto-2021/","publishdate":"2021-10-06T14:36:35.998652Z","relpermalink":"/publication/conference-paper-tto-2021/","section":"publication","summary":"Automated fact-checking (AFC) systems exist to combat disinformation, however their complexity makes them opaque to the end user, making it difficult to foster trust. In this paper, we introduce the E-BART model with the hope of making progress on this front. E-BART is able to provide a veracity prediction for a claim, and jointly generate a human-readable explanation for this decision. We show that E-BART is competitive with the state-of-theart on the e-FEVER and e-SNLI tasks. In addition, we validate the joint-prediction architecture by showing 1) that generating explanations does not significantly impede the model from performing well in its main task of veracity prediction, and 2) that predicted veracity and explanations are more internally coherent when generated jointly than separately. Finally, we also conduct human evaluations on the impact of generated explanations and observe that explanations increase human ability to spot misinformation and make people more skeptical about claims.","tags":null,"title":"E-BART: Jointly Predicting and Explaining Truthfulness","type":"publication"},{"authors":["Kevin Roitero","Michael Soprano","Beatrice Portelli","Massimiliano De Luise","Vincenzo Della Mea","Giuseppe Serra","Stefano Mizzaro","Gianluca Demartini"],"categories":[],"content":"","date":1631750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635170515,"objectID":"d39e4e4a8fad306722a6298f1ae8170a","permalink":"https://michaelsoprano.com/publication/journal-paper-puc-2021/","publishdate":"2021-09-15T15:00:00+01:00","relpermalink":"/publication/journal-paper-puc-2021/","section":"publication","summary":"Recently, the misinformation problem has been addressed with a crowdsourcing-based approach: to assess the truthfulness of a statement, instead of relying on a few experts, a crowd of non-expert is exploited. We study whether crowdsourcing is an effective and reliable method to assess truthfulness during a pandemic, targeting statements related to COVID-19, thus addressing (mis)information that is both related to a sensitive and personal issue and very recent as compared to when the judgment is done. In our experiments, crowd workers are asked to assess the truthfulness of statements, and to provide evidence for the assessments. Besides showing that the crowd is able to accurately judge the truthfulness of the statements, we report results on workers behavior, agreement among workers, effect of aggregation functions, of scales transformations, and of workers background and bias. We perform a longitudinal study by re-launching the task multiple times with both novice and experienced workers, deriving important insights on how the behavior and quality change over time. Our results show that workers are able to detect and objectively categorize online (mis)information related to COVID-19; both crowdsourced and expert judgments can be transformed and aggregated to improve quality; worker background and other signals (e.g., source of information, behavior) impact the quality of the data. The longitudinal study demonstrates that the time-span has a major effect on the quality of the judgments, for both novice and experienced workers. Finally, we provide an extensive failure analysis of the statements misjudged by the crowd-workers.","tags":["truthfulness","crowdsourcing","misinformation","covid"],"title":"Can The Crowd Judge Truthfulness? A Longitudinal Study on Recent Misinformation About COVID-19","type":"publication"},{"authors":["Davide Ceolin","Giuseppe Primiero","Jan Wielemaker","Michael Soprano"],"categories":[],"content":"","date":1620691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623417220,"objectID":"2d5d3cbe6fbf6fa476bdcfa1ff8da2d0","permalink":"https://michaelsoprano.com/publication/conference-paper-icwe-2021/","publishdate":"2021-05-11T14:00:00Z","relpermalink":"/publication/conference-paper-icwe-2021/","section":"publication","summary":"Review scores collect users opinions in a simple and intuitive manner. However, review scores are also easily manipulable, hence they are often accompanied by explanations. A substantial amount of research has been devoted to ascertaining the quality of reviews, to identify the most useful and authentic scores through explanation analysis. In this paper, we advance the state of the art in review quality analysis. We introduce a rating system to identify review arguments and to define an appropriate weighted semantics through formal argumentation theory. We introduce an algorithm to construct a corresponding graph, based on a selection of weighted arguments, their semantic similarity, and the supported ratings. We provide an algorithm to identify the model of such an argumentation graph, maximizing the overall weight of the admitted nodes and edges. We evaluate these contributions on the Amazon review dataset by McAuley et al.  [15], by comparing the results of our argumentation assessment with the upvotes received by the reviews. Also, we deepen the evaluation by crowdsourcing a multidimensional assessment of reviews and comparing it to the argumentation assessment. Lastly, we perform a user study to evaluate the explainability of our method. Our method achieves two goals: (1) it identifies reviews that are considered useful, comprehensible, truthful by online users and does so in an unsupervised manner, and (2) it provides an explanation of quality assessments.","tags":["formal argumentation theory","online reviews","information quality"],"title":"Assessing the Quality of Online Reviews Using Formal Argumentation Theory","type":"publication"},{"authors":["Michael Soprano"],"categories":["teaching"],"content":"Teachers  Stefano Mizzaro - Theory Michael Soprano - Laboratory  Aims The aim of the course is to provide to the student the foundational knowledge and the practical skills concerning the area of social informatics. The course discusses both the so-called Social Media (Facebook, Twitter, etc.) and the Crowdsourcing phenomenon. In the first case, social behavior is supported by computational systems; in the second case, computational systems are supported by social behavior. The course deals with conceptual topics, theoretical foundations, and practical applications.\nTable of Contents My contribution to the course consisted in teaching practical applications of the concepts outlined by prof. Stefano Mizzaro for a total of 6 lectures (12 hours). The course\u0026rsquo;s language is Italian.\n Lesson 1: Twitter API: Introduzione \u0026amp; Richiami Lesson 2: Twitter API: Tweepy \u0026amp; Requests Lesson 3: Grafi: NetworkX Lesson 4: Crowd_Frame: How To  ","date":1615892400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615898929,"objectID":"95c36c21ea72ea7890dd0b4aeab82a0b","permalink":"https://michaelsoprano.com/post/social-computing-2020_2021/","publishdate":"2021-03-16T11:00:00Z","relpermalink":"/post/social-computing-2020_2021/","section":"post","summary":"Bachelor's Degree in Internet of Things, Big Data \u0026 Web at the University of Udine. Academic Year 2020/2021. Lectures: 6. Hours: 12","tags":["Python","Jupyter","Twitter API","Tweepy","Postman","Graph","NetworkX","PyViz","Amazon Mechanical Turk","Angular","Crowdsourcing","Data Analysis"],"title":"Social Computing - 2020/2021","type":"post"},{"authors":["Michael Soprano"],"categories":null,"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615829732,"objectID":"e280df8a5bf2b13f4f946322a0d9f695","permalink":"https://michaelsoprano.com/talk/the-covid-19-infodemic-can-the-crowd-judge-recent-misinformation-objectively/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/talk/the-covid-19-infodemic-can-the-crowd-judge-recent-misinformation-objectively/","section":"event","summary":"Conference Talk - The 29th ACM International Conference on Information and Knowledge Management (CIKM 2020). October 19-23, 2020. Galway, Ireland. Online. Held remotely due to COVID-19 (pre-recorded contribution).","tags":["covid-19","crowdsourcing"],"title":"The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation Objectively?","type":"event"},{"authors":["Kevin Roitero","Michael Soprano","Beatrice Portelli","Damiano Spina","Vincenzo Della Mea","Giuseppe Serra","Stefano Mizzaro","Gianluca Demartini"],"categories":[],"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"b7c6cb68071cb5ffba17721e2afe0c55","permalink":"https://michaelsoprano.com/publication/conference-paper-cikm-2020/","publishdate":"2021-03-15T14:36:35.536697Z","relpermalink":"/publication/conference-paper-cikm-2020/","section":"publication","summary":"Misinformation is an ever increasing problem that is difficult to solve for the research community and has a negative impact on the society at large. Very recently, the problem has been addressed with a crowdsourcing-based approach to scale up labeling efforts: to assess the truthfulness of a statement, instead of relying on a few experts, a crowd of (non-expert) judges is exploited. We follow the same approach to study whether crowdsourcing is an effective and reliable method to assess statements truthfulness during a pandemic. We specifically target statements related to the COVID-19 health emergency, that is still ongoing at the time of the study and has arguably caused an increase of the amount of misinformation that is spreading online (a phenomenon for which the term \\\"infodemic\\\" has been used). By doing so, we are able to address (mis)information that is both related to a sensitive and personal issue like health and very recent as compared to when the judgment is done: two issues that have not been analyzed in related work.In our experiment, crowd workers are asked to assess the truthfulness of statements, as well as to provide evidence for the assessments as a URL and a text justification. Besides showing that the crowd is able to accurately judge the truthfulness of the statements, we also report results on many different aspects, including: agreement among workers, the effect of different aggregation functions, of scales transformations, and of workers background / bias. We also analyze workers behavior, in terms of queries submitted, URLs found / selected, text justifications, and other behavioral data like clicks and mouse actions collected by means of an ad hoc logger.","tags":["ordinal classification","infodemic","information behavior","covid-19","crowdsourcing"],"title":"The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation Objectively?","type":"publication"},{"authors":["Kevin Roitero","Michael Soprano","Shaoyang Fan","Damiano Spina","Stefano Mizzaro","Gianluca Demartini"],"categories":[],"content":"","date":1595635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"390c638489d322ff5d76f2352ebd6880","permalink":"https://michaelsoprano.com/publication/conference-paper-sigir-2020/","publishdate":"2021-03-15T14:36:35.998652Z","relpermalink":"/publication/conference-paper-sigir-2020/","section":"publication","summary":"Truthfulness judgments are a fundamental step in the process of fighting misinformation, as they are crucial to train and evaluate classifiers that automatically distinguish true and false statements. Usually such judgments are made by experts, like journalists for political statements or medical doctors for medical statements. In this paper, we follow a different approach and rely on (non-expert) crowd workers. This of course leads to the following research question: Can crowdsourcing be reliably used to assess the truthfulness of information and to create large-scale labeled collections for information credibility systems? To address this issue, we present the results of an extensive study based on crowdsourcing: we collect thousands of truthfulness assessments over two datasets, and we compare expert judgments with crowd judgments, expressed on scales with various granularity levels. We also measure the political bias and the cognitive background of the workers, and quantify their effect on the reliability of the data provided by the crowd.","tags":["classification","crowdsourcing","information credibility"],"title":"Can The Crowd Identify Misinformation Objectively? The Effects of Judgment Scale and Assessor's Background","type":"publication"},{"authors":["Michael Soprano"],"categories":null,"content":"","date":1595635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"44da9a7f22f7ab656a77d63742ac41fe","permalink":"https://michaelsoprano.com/talk/can-the-crowd-identify-misinformation-objectively-the-effects-of-judgment-scale-and-assessors-background./","publishdate":"2020-07-25T00:00:00Z","relpermalink":"/talk/can-the-crowd-identify-misinformation-objectively-the-effects-of-judgment-scale-and-assessors-background./","section":"event","summary":"Conference Talk - The 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Held remotely due to COVID-19 (pre-recorded contribution).","tags":["disinformation","misinformation","crowdsourcing"],"title":"Can The Crowd Identify Misinformation Objectively? The Effects of Judgment Scale and Assessor’s Background.","type":"event"},{"authors":["Michael Soprano"],"categories":null,"content":"","date":1568592e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615829732,"objectID":"b98a09de373c0ca77ce37253cd54a086","permalink":"https://michaelsoprano.com/talk/bias-and-fairness-in-effectiveness-evaluation-by-means-of-network-analysis-and-mixture-model/","publishdate":"2019-09-16T00:00:00Z","relpermalink":"/talk/bias-and-fairness-in-effectiveness-evaluation-by-means-of-network-analysis-and-mixture-model/","section":"event","summary":"Workshop Talk - Italian Information Retrieval Workshop (IIR 2019). September 16-18, 2019. Padova, Italy.","tags":["bias","fairness","evaluation"],"title":"Bias and Fairness in Effectiveness Evaluation by Means of Network Analysis and Mixture Model","type":"event"},{"authors":["Michael Soprano","Kevin Roitero","Stefano Mizzaro"],"categories":[],"content":"","date":1568592e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"a8d187a3d1f0e575d6b3d060bc6fc815","permalink":"https://michaelsoprano.com/publication/workshop-paper-iir-2019/","publishdate":"2021-03-15T14:36:36.438925Z","relpermalink":"/publication/workshop-paper-iir-2019/","section":"publication","summary":"Information retrieval effectiveness evaluation is often carried out by means of test collections. Many works investigated possible sources of bias in such an approach. We propose a systematic approach to identify bias and its causes, and to remove it, thus enforcing fairness in effectiveness evaluation by means of test collections.","tags":["test collection","bias","fairness","network analysis"],"title":"Bias and Fairness in Effectiveness Evaluation by Means of Network Analysis and Mixture Models","type":"publication"},{"authors":["Michael Soprano","Kevin Roitero","Stefano Mizzaro"],"categories":[],"content":"","date":1564012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"174c9ca5c39cc4b0e4cb9a7b075950d1","permalink":"https://michaelsoprano.com/publication/workshop-paper-birnld-2019/","publishdate":"2021-03-15T14:36:36.88953Z","relpermalink":"/publication/workshop-paper-birnld-2019/","section":"publication","summary":"Peer review is a well known mechanism exploited within the scholarly publishing process to ensure the quality of scientific literature. Such a mechanism, despite being well established and reasonable, is not free from problems, and alternative approaches to peer review have been developed. Such approaches exploit the readers of scientific publications and their opinions, and thus outsource the peer review activity to the scholar community; an example of this approach has been formalized in the Readersourcing model [5]. Our contribution is two-fold:(i) we propose a stochastic validation of the Readersourcing model, and (ii) we employ network analysis techniques to study the bias of the model, and in particular the interactions between readers and papers and their goodness and effectiveness scores. Our results show that by using network analysis interesting model properties can be derived.","tags":["peer review","network analysis","hits"],"title":"HITS Hits Readersourcing: Validating Peer Review Alternatives Using Network Analysis","type":"publication"},{"authors":["Michael Soprano","Stefano Mizzaro"],"categories":[],"content":"","date":1548892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647625505,"objectID":"257fb4e5e2639d89aabf652f90a7a6e2","permalink":"https://michaelsoprano.com/publication/conference-paper-ircdl-2019/","publishdate":"2021-03-15T14:36:37.331078Z","relpermalink":"/publication/conference-paper-ircdl-2019/","section":"publication","summary":"This paper describes Readersourcing 2.0, an ecosystem providing an implementation of the Readersourcing approach proposed by Mizzaro [10]. Readersourcing is proposed as an alternative to the standard peer review activity that aims to exploit the otherwise lost opinions of readers. Readersourcing 2.0 implements two different models based on the so-called codetermination algorithms. We describe the requirements, present the overall architecture, and show how the end-user can interact with the system. Readersourcing 2.0 will be used in the future to study also other topics, like the idea of shepherding the users to achieve a better quality of the reviews and the differences between a review activity carried out with a single-blind or a double-blind approach.","tags":["scholarly publishing","peer review","crowdsourcing"],"title":"Crowdsourcing Peer Review: As We May Do","type":"publication"},{"authors":["Michael Soprano","Stefano Mizzaro"],"categories":[],"content":"","date":1548201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"4418b58d9df2b2fbe71b648dfddc3b83","permalink":"https://michaelsoprano.com/publication/conference-paper-aiudc-2019/","publishdate":"2021-03-15T14:36:37.769985Z","relpermalink":"/publication/conference-paper-aiudc-2019/","section":"publication","summary":"We propose an alternative approach to the standard peer review activity that aims to exploit the otherwise lost opinions of readers of publications which is called Readersourcing, originally proposed by Mizzaro [1]. Such an approach can be formalized by means of different models which share the same general principles. These models should be able to define a way, to measure the overall quality of a publication as well the reputation of a reader as an assessor; moreover, from these measures it should be possible to derive the reputation of a scholar as an author. We describe an ecosystem called Readersourcing 2.0 which provides an implementation for two Readersourcing models [2, 3] by outlining its goals and requirements. Readersourcing 2.0 will be used in the future to gather fresh data to analyze and validate.","tags":["scholarly publishing","peer review","crowdsourcing"],"title":"Crowdsourcing Peer Review in the Digital Humanities?","type":"publication"},{"authors":["Kevin Roitero","Michael Soprano","Andrea Brunello","Stefano Mizzaro"],"categories":[],"content":"","date":153576e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"1c2648a6ba53773cf225e3ddddb6e0a7","permalink":"https://michaelsoprano.com/publication/journal-paper-jdiq-2018/","publishdate":"2021-03-15T14:36:38.653339Z","relpermalink":"/publication/journal-paper-jdiq-2018/","section":"publication","summary":"Effectiveness evaluation of information retrieval systems by means of a test collection is a widely used methodology. However, it is rather expensive in terms of resources, time, and money; therefore, many researchers have proposed methods for a cheaper evaluation. One particular approach, on which we focus in this article, is to use fewer topics: in TREC-like initiatives, usually system effectiveness is evaluated as the average effectiveness on a set of n topics (usually, n=50, but more than 1,000 have been also adopted); instead of using the full set, it has been proposed to find the best subsets of a few good topics that evaluate the systems in the most similar way to the full set. The computational complexity of the task has so far limited the analysis that has been performed. We develop a novel and efficient approach based on a multi-objective evolutionary algorithm. The higher efficiency of our new implementation allows us to reproduce some notable results on topic set reduction, as well as perform new experiments to generalize and improve such results. We show that our approach is able to both reproduce the main state-of-the-art results and to allow us to analyze the effect of the collection, metric, and pool depth used for the evaluation. Finally, differently from previous studies, which have been mainly theoretical, we are also able to discuss some practical topic selection strategies, integrating results of automatic evaluation approaches.","tags":["topic selection strategy","evolutionary algorithms","reproducibility","test collection","few topics","topic sets"],"title":"Reproduce and Improve: An Evolutionary Approach to Select a Few Good Topics for Information Retrieval Evaluation","type":"publication"},{"authors":["Kevin Roitero","Michael Soprano","Stefano Mizzaro"],"categories":[],"content":"","date":1531008e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621603946,"objectID":"725560812dc18a4727af0d86074c5700","permalink":"https://michaelsoprano.com/publication/conference-paper-sigir-2018/","publishdate":"2021-03-15T14:36:38.202502Z","relpermalink":"/publication/conference-paper-sigir-2018/","section":"publication","summary":"Several researchers have proposed to reduce the number of topics used in TREC-like initiatives. One research direction that has been pursued is what is the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way. Such a research direction has been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. We propose such a practical criterion for topic selection: we rely on the methods for automatic system evaluation without relevance judgments, and by running some experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than random topics.","tags":["topic selection","trec","few topics","test collections"],"title":"Effectiveness Evaluation with a Subset of Topics: A Practical Approach","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615470069,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://michaelsoprano.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"The site is completely static, no use of server-side cookies is made and the only information that might be stored is your choice to see the page in the night mode.\nTherefore there’s no harm to your privacy, unless you explicitly contact me by email or through a social network.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615898929,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://michaelsoprano.com/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"The site is completely static, no use of server-side cookies is made and the only information that might be stored is your choice to see the page in the night mode.","tags":null,"title":"Privacy Policy","type":"page"}]