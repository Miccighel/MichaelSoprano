@article{10.1145/3674884,
    title        = {{Longitudinal Loyalty: Understanding The Barriers To Running Longitudinal Studies On Crowdsourcing Platforms}},
    author       = {Soprano, Michael and Roitero, Kevin and Gadiraju, Ujwal and Maddalena, Eddy and Demartini, Gianluca},
    year         = 2024,
    month        = 7,
    journal      = {ACM Transactions on Social Computing},
    publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
    doi          = {10.1145/3674884},
    url          = {https://doi.org/10.1145/3674884},
    note         = {Journal Rank: Scimago (SJR) Q2 (2023)},
    abstract     = {Crowdsourcing tasks have been widely used to collect a large number of human labels at scale. While some of these tasks are deployed by requesters and performed only once by crowd workers, others require the same worker to perform the same task or a variant of it more than once, thus participating in a so-called longitudinal study. Despite the prevalence of longitudinal studies in crowdsourcing, there is a limited understanding of factors that influence worker participation in them across different crowdsourcing marketplaces. We present results from a large-scale survey of 300 workers on 3 different micro-task crowdsourcing platforms: Amazon Mechanical Turk, Prolific and Toloka. The aim is to understand how longitudinal studies are performed using crowdsourcing. We collect answers about 547 experiences and we analyze them both quantitatively and qualitatively. We synthesize 17 take-home messages about longitudinal studies together with 8 recommendations for task requesters and 5 best practices for crowdsourcing platforms to adequately conduct and support such kinds of studies. We release the survey and the data at: https://osf.io/h4du9/.},
    keywords     = {Longitudinal Studies, Crowdsourcing Platforms, Surveys, Online Sampling, Amazon Mechanical Turk, Prolific, Toloka}
}