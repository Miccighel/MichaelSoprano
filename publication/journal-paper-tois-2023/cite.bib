@article{10.1145/3597201,
  title = {{How Many Crowd Workers Do I Need? On Statistical Power When Crowdsourcing Relevance Judgments}},
  author = {Roitero, Kevin and Barbera, David La and Soprano, Michael and Demartini, Gianluca and Mizzaro, Stefano and Sakai, Tetsuya},
  year = 2023,
  month = 5,
  journal = {ACM Transactions on Information Systems},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3597201},
  issn = {1046-8188},
  note = {Just Accepted},
  abstract = {To scale the size of Information Retrieval collections, crowdsourcing has become a common way to collect relevance judgments at scale. Crowdsourcing experiments usually employ 100-10,000 workers, but such a number is often decided in a heuristic way. The downside is that the resulting dataset does not have any guarantee of meeting predefined statistical requirements as, for example, have enough statistical power to be able to distinguish in a statistically significant way between the relevance of two documents. We propose a methodology adapted from literature on sound topic set size design, based on t-test and ANOVA, which aims at guaranteeing the resulting dataset to meet a predefined set of statistical requirements. We validate our approach on several public datasets. Our results show that we can reliably estimate the recommended number of workers needed to achieve statistical power, and that such estimation is dependent on the topic, while the effect of the relevance scale is limited. Furthermore, we found that such estimation is dependent on worker features such as agreement. Finally, we describe a set of practical estimation strategies that can be used to estimate the worker set size, and we also provide results on the estimation of document set sizes.},
  keywords = {relevance judgments, statistical analysis, crowdsourcing}
}
