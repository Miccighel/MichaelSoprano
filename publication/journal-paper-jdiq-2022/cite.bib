@article{10.1145/3546917,
  author = {Brand, Erik and Roitero, Kevin and Soprano, Michael and Rahimi, Afshin and Demartini, Gianluca},
  title = {A Neural Model to Jointly Predict and Explain Truthfulness of Statements},
  year = {2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {15},
  number = {1},
  issn = {1936-1955},
  doi = {10.1145/3546917},
  abstract = {Automated fact-checking (AFC) systems exist to combat disinformation, however, their complexity usually makes them opaque to the end-user, making it difficult to foster trust in the system. In this article, we introduce the E-BART model with the hope of making progress on this front. E-BART is able to provide a veracity prediction for a claim and jointly generate a human-readable explanation for this decision. We show that E-BART is competitive with the state-of-the-art on the e-FEVER and e-SNLI tasks. In addition, we validate the joint-prediction architecture by showing (1) that generating explanations does not significantly impede the model from performing well in its main task of veracity prediction, and (2) that predicted veracity and explanations are more internally coherent when generated jointly than separately. We also calibrate the E-BART model, allowing the output of the final model to be correctly interpreted as the confidence of correctness. Finally, we also conduct an extensive human evaluation on the impact of generated explanations and observe that: Explanations increase human ability to spot misinformation and make people more skeptical about claims, and explanations generated by E-BART are competitive with ground truth explanations.},
  journal = {Journal of Data and Information Quality},
  month = {12},
  articleno = {4},
  numpages = {19},
  keywords = {explainable artificial intelligence, misinformation}
}
