---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Can The Crowd Identify Misinformation Objectively? The Effects of Judgment
  Scale and Assessor's Background
subtitle: ''
summary: ''
authors:
- Kevin Roitero
- Michael Soprano
- Shaoyang Fan
- Damiano Spina
- Stefano Mizzaro
- Gianluca Demartini
tags:
- classification
- crowdsourcing
- information credibility
categories: []
date: '2020-07-25'
lastmod: 2021-03-15T15:36:36+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-03-15T14:36:35.998652Z'
publication_types:
- '1'
abstract: 'Truthfulness judgments are a fundamental step in the process of fighting
  misinformation, as they are crucial to train and evaluate classifiers that automatically
  distinguish true and false statements. Usually such judgments are made by experts,
  like journalists for political statements or medical doctors for medical statements.
  In this paper, we follow a different approach and rely on (non-expert) crowd workers.
  This of course leads to the following research question: Can crowdsourcing be reliably
  used to assess the truthfulness of information and to create large-scale labeled
  collections for information credibility systems? To address this issue, we present
  the results of an extensive study based on crowdsourcing: we collect thousands of
  truthfulness assessments over two datasets, and we compare expert judgments with
  crowd judgments, expressed on scales with various granularity levels. We also measure
  the political bias and the cognitive background of the workers, and quantify their
  effect on the reliability of the data provided by the crowd.'
publication: '*Proceedings of the 43st International ACM SIGIR Conference on Research and Development in Information Retrieval. Conference Rank: GGS A++, Core A.*'
doi: 10.1145/3397271.3401112
---
