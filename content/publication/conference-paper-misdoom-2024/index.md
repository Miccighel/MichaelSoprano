---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Crowdsourcing Statement Classification to Enhance Information Quality Prediction'
subtitle: ''
summary: ''
authors:
- Jaspreet Singh
- Michael Soprano
- Kevin Roitero
- Davide Ceolin
tags:
- crowdsourcing annotation
- information quality assessment
- argument type identification
categories: []
date: '2024-09-01'
lastmod: 2024-09-01T10:00:00+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2024-09-01T10:00:00:35.998652Z'
publication_types:
- '1'
abstract: 'This paper explores the use of crowdsourcing to classify statement types in film reviews to assess their information quality. Employing the Argument Type Identification Procedure which uses the Periodic Table of Arguments to categorize arguments, the study aims to connect statement types to the overall argument strength and information reliability. Focusing on non-expert annotators in a crowdsourcing environment, the research assesses their reliability based on various factors including language proficiency and annotation experience. Results indicate the importance of careful annotator selection and training to achieve high inter-annotator agreement and highlight challenges in crowdsourcing statement classification for information quality assessment.'
publication: '*Disinformation in Open Online Media - 6th Multidisciplinary International Symposium (MISDOOM 2024). MÃ¼nster, Germany.*'
---
