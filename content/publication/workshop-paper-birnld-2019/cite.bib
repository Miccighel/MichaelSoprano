@inproceedings{workshop-paper-birnld2019,
    title        = {{HITS Hits Readersourcing: Validating Peer Review Alternatives Using Network Analysis}},
    author       = {Soprano, Michael and Roitero, Kevin and Mizzaro, Stefano},
    year         = 2019,
    month        = 7,
    booktitle    = {{Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval}},
    location     = {Paris, France},
    publisher    = {{CEUR-WS.org}},
    series       = {{CEUR Workshop Proceedings}},
    volume       = {{2414}},
    pages        = {{70--82}},
    url          = {{http://ceur-ws.org/Vol-2414/paper7.pdf}},
    editor       = {{Chandrasekaran, Muthu Kumar and Mayr, Philipp}},
    keywords     = {{peer review, network analysis, hits}},
    abstract     = {{Peer review is a well known mechanism exploited within the scholarly publishing process to ensure the quality of scientific literature. Such a mechanism, despite being well established and reasonable, is not free from problems, and alternative approaches to peer review have been developed. Such approaches exploit the readers of scientific publications and their opinions, and thus outsource the peer review activity to the scholar community; an example of this approach has been formalized in the Readersourcing model [5]. Our contribution is two-fold:(i) we propose a stochastic validation of the Readersourcing model, and (ii) we employ network analysis techniques to study the bias of the model, and in particular the interactions between readers and papers and their goodness and effectiveness scores. Our results show that by using network analysis interesting model properties can be derived.}}
}
