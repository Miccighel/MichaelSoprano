---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Assessing the Quality of Online Reviews Using Formal Argumentation Theory'
subtitle: ''
summary: ''
authors:
- Davide Ceolin
- Giuseppe Primiero
- Jan Wielemaker
- Michael Soprano 
tags:
- formal argumentation theory
- online reviews
- information quality 
categories: []
date: '2021-05-11'
lastmod: 2021-05-11T14:00:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
caption: ''
focal_point: ''
preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-05-11T14:00:00'
publication_types:
- '1' 
abstract: 'Review scores collect users opinions in a simple and intuitive manner. However, review scores are also
  easily manipulable, hence they are often accompanied by explanations. A substantial amount of research has
  been devoted to ascertaining the quality of reviews, to identify the most useful and authentic scores through
  explanation analysis. In this paper, we advance the state of the art in review quality analysis. We introduce a
  rating system to identify review arguments and to define an appropriate weighted semantics through formal argumentation
  theory. We introduce an algorithm to construct a corresponding graph, based on a selection of weighted arguments,
  their semantic similarity, and the supported ratings. We provide an algorithm to identify the model of such an
  argumentation graph, maximizing the overall weight of the admitted nodes and edges. We evaluate these contributions
  on the Amazon review dataset by McAuley et al.Â  [15], by comparing the results of our argumentation assessment with
  the upvotes received by the reviews. Also, we deepen the evaluation by crowdsourcing a multidimensional assessment of
  reviews and comparing it to the argumentation assessment. Lastly, we perform a user study to evaluate the explainability
  of our method. Our method achieves two goals: (1) it identifies reviews that are considered useful, comprehensible,
  truthful by online users and does so in an unsupervised manner, and (2) it provides an explanation of quality assessments.' 
publication: '*Proceedings of the 21st International Conference on Web Engineering (ICWE 2021). Biarritz, France (Online).
  May 18-21, 2021. Conference Rank: GGS B, Core B*'
doi: 10.1007/978-3-030-74296-6_6
---
